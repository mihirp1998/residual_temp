{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dataset\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as LS\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.utils import _pair\n",
    "\n",
    "\n",
    "class ConvRNNCellBase(nn.Module):\n",
    "    def __repr__(self):\n",
    "        s = (\n",
    "            '{name}({input_channels}, {hidden_channels}, kernel_size={kernel_size}'\n",
    "            ', stride={stride}')\n",
    "        if self.padding != (0, ) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1, ) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        s += ', hidden_kernel_size={hidden_kernel_size}'\n",
    "        s += ')'\n",
    "        return s.format(name=self.__class__.__name__, **self.__dict__)\n",
    "\n",
    "\n",
    "class ConvLSTMCell(ConvRNNCellBase):\n",
    "    def __init__(self,\n",
    "                 input_channels,\n",
    "                 hidden_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 padding=0,\n",
    "                 dilation=1,\n",
    "                 hidden_kernel_size=1,\n",
    "                 bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.kernel_size = _pair(kernel_size)\n",
    "        self.stride = _pair(stride)\n",
    "        self.padding = _pair(padding)\n",
    "        self.dilation = _pair(dilation)\n",
    "\n",
    "        self.hidden_kernel_size = _pair(hidden_kernel_size)\n",
    "\n",
    "        hidden_padding = _pair(hidden_kernel_size // 2)\n",
    "\n",
    "        gate_channels = 4 * self.hidden_channels\n",
    "        self.conv_ih = nn.Conv2d(\n",
    "            in_channels=self.input_channels,\n",
    "            out_channels=gate_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "            dilation=self.dilation,\n",
    "            bias=bias)\n",
    "\n",
    "        self.conv_hh = nn.Conv2d(\n",
    "            in_channels=self.hidden_channels,\n",
    "            out_channels=gate_channels,\n",
    "            kernel_size=hidden_kernel_size,\n",
    "            stride=1,\n",
    "            padding=hidden_padding,\n",
    "            dilation=1,\n",
    "            bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv_ih.reset_parameters()\n",
    "        self.conv_hh.reset_parameters()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        hx, cx = hidden\n",
    "        gates = self.conv_ih(input) + self.conv_hh(hx)\n",
    "#         print(\"gates\",gates.shape)\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "\n",
    "        ingate = F.sigmoid(ingate)\n",
    "        forgetgate = F.sigmoid(forgetgate)\n",
    "        cellgate = F.tanh(cellgate)\n",
    "        outgate = F.sigmoid(outgate)\n",
    "\n",
    "        cy = (forgetgate * cx) + (ingate * cellgate)\n",
    "        hy = outgate * F.tanh(cy)\n",
    "\n",
    "        return hy, cy\n",
    "\n",
    "\n",
    "class HyperConvLSTMCell(ConvRNNCellBase):\n",
    "    def __init__(self,input_channels,main_num_units,hyper_unit,context_input_channels,hyper_embedding = 128):\n",
    "        super(HyperConvLSTMCell, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.num_units = main_num_units\n",
    "        self.hyper_num_unit = hyper_unit\n",
    "        self.hyper_embedding =hyper_embedding\n",
    "        self.gate_params  = self.num_units * 4\n",
    "        self.context_input_channels = context_input_channels\n",
    "        self.hyper_input_units = context_input_channels+ main_num_units  \n",
    "\n",
    " \n",
    "        # print(self.hyper_input_units,self.hyper_num_unit)\n",
    "\n",
    "        self.hyper_cell = ConvLSTMCell(\n",
    "            self.hyper_input_units,\n",
    "            self.hyper_num_unit,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            hidden_kernel_size=1,\n",
    "            bias=False)\n",
    "        \n",
    "        self.hyper_cell = self.hyper_cell.cuda()\n",
    "\n",
    "#         self.temp_conv = nn.Conv2d(\n",
    "#                     input_channels , \n",
    "#                     input_channels, \n",
    "#                     kernel_size=3, stride=2, padding=1, bias=False)\n",
    "#         self.temp_conv = self.temp_conv.cuda()\n",
    "\n",
    "        self.conv_z_input  = nn.Conv2d(self.hyper_num_unit, self.hyper_embedding, \n",
    "                        kernel_size=1, stride=1, padding=0, bias=False).cuda()\n",
    "\n",
    "        self.conv_z_state  = nn.Conv2d(self.hyper_num_unit, self.hyper_embedding, \n",
    "                        kernel_size=1, stride=1, padding=0, bias=False).cuda()\n",
    "\n",
    "        self.conv_alpha_input  = nn.Conv2d(self.hyper_embedding , self.gate_params, \n",
    "                    kernel_size=1, stride=1, padding=0, bias=False).cuda()\n",
    "\n",
    "        self.conv_alpha_state  = nn.Conv2d(self.hyper_embedding , self.gate_params, \n",
    "                    kernel_size=1, stride=1, padding=0, bias=False).cuda()\n",
    "\n",
    "        \n",
    "        \n",
    "        self.conv_transform_gates_input  = nn.Conv2d(self.input_channels , self.gate_params, \n",
    "                    kernel_size=3, stride=1, padding=1, bias=False).cuda()\n",
    "\n",
    "        self.conv_transform_gates_states  = nn.Conv2d(self.num_units , self.gate_params, \n",
    "                    kernel_size=1, stride=1, padding=0, bias=False).cuda()\n",
    "\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.hyper_cell.reset_parameters()\n",
    "        self.conv_z_input.reset_parameters()\n",
    "        self.conv_z_state.reset_parameters()\n",
    "        self.conv_alpha_input.reset_parameters()\n",
    "        self.conv_alpha_state.reset_parameters()\n",
    "        self.conv_transform_gates_input.reset_parameters()\n",
    "        self.conv_transform_gates_states.reset_parameters()\n",
    "\n",
    "    def hyper_norm_input(self,input_layer,hyper_h):\n",
    "        zw = self.conv_z_input(hyper_h)\n",
    "        alpha = self.conv_alpha_input(zw)\n",
    "\n",
    "        result = input_layer * alpha\n",
    "        \n",
    "        return result\n",
    "\n",
    "    \n",
    "    def hyper_norm_state(self,input_layer,hyper_h):\n",
    "        zw = self.conv_z_state(hyper_h)\n",
    "        \n",
    "        alpha = self.conv_alpha_state(zw)\n",
    "        result = input_layer * alpha\n",
    "        \n",
    "        return result    \n",
    "    # def reset_parameters(self):\n",
    "    #     self.conv_ih.reset_parameters()\n",
    "    #     self.conv_hh.reset_parameters()\n",
    "\n",
    "    def forward(self, input,context, hidden):\n",
    "        h,c = hidden\n",
    "        main_h = h[:,:self.num_units]\n",
    "        main_c = c[:,:self.num_units]\n",
    "        hyper_h = h[:,self.num_units:]\n",
    "        hyper_c = h[:,self.num_units:]\n",
    "        # print(\"input shape \",input.shape)\n",
    "        hyper_states = (hyper_h,hyper_c)\n",
    "        # if self.encoder:\n",
    "        #     input = self.temp_conv(input)\n",
    "        hyper_input = torch.cat([context,main_h],dim=1)\n",
    "        # print(\"hyper shape \",hyper_input.shape,hyper_states[0].shape)\n",
    "\n",
    "        #print(hyper_input.shape,hyper_states[0].shape)\n",
    "        hyper_h,hyper_c = self.hyper_cell(hyper_input,hyper_states)\n",
    "\n",
    "        input_below_ = self.conv_transform_gates_input(input)\n",
    "        input_below_ = self.hyper_norm_input(input_below_,hyper_h)\n",
    "\n",
    "        state_below_ = self.conv_transform_gates_states(main_h)\n",
    "        state_below_ = self.hyper_norm_state(state_below_,hyper_h)\n",
    "\n",
    "        lstm_matrix = input_below_ + state_below_\n",
    "\n",
    "        i,j,f,o = lstm_matrix.chunk(4,1)\n",
    "\n",
    "\n",
    "        new_main_c = (self.sigmoid(f)*main_c) + (self.sigmoid(i)*self.tanh(j))\n",
    "        new_main_h = self.tanh(new_main_c)* self.sigmoid(o)\n",
    "\n",
    "        new_total_h =torch.cat([new_main_h,hyper_h],dim=1)\n",
    "\n",
    "        new_total_c = torch.cat([new_main_c,hyper_c],dim=1)\n",
    "\n",
    "\n",
    "        return (new_total_h,new_total_c),new_main_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from modules import Sign\n",
    "\n",
    "\n",
    "class EncoderCell(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCell, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.rnn1 = ConvLSTMCell(\n",
    "            64,\n",
    "            256,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            hidden_kernel_size=1,\n",
    "            bias=False)\n",
    "        self.rnn2 = ConvLSTMCell(\n",
    "            256,\n",
    "            512,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            hidden_kernel_size=1,\n",
    "            bias=False)\n",
    "        self.rnn3 = ConvLSTMCell(\n",
    "            512,\n",
    "            512,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            hidden_kernel_size=1,\n",
    "            bias=False)\n",
    "\n",
    "    def forward(self, input, hidden1, hidden2, hidden3):\n",
    "        x = self.conv(input)\n",
    "#         print(x.shape)\n",
    "        hidden1 = self.rnn1(x, hidden1)\n",
    "        x = hidden1[0]\n",
    "\n",
    "        hidden2 = self.rnn2(x, hidden2)\n",
    "        x = hidden2[0]\n",
    "\n",
    "        hidden3 = self.rnn3(x, hidden3)\n",
    "        x = hidden3[0]\n",
    "\n",
    "        return x, hidden1, hidden2, hidden3\n",
    "\n",
    "\n",
    "class Binarizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Binarizer, self).__init__()\n",
    "        self.conv = nn.Conv2d(512, 32, kernel_size=1, bias=False)\n",
    "        self.sign = Sign()\n",
    "\n",
    "    def forward(self, input):\n",
    "        feat = self.conv(input)\n",
    "        x = F.tanh(feat)\n",
    "        return self.sign(x)\n",
    "    \n",
    "# input_channels,main_num_units,hyper_unit,context_input_channels\n",
    "\n",
    "class DecoderCell(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderCell, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            32, 512, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.rnn1 = ConvLSTMCell(\n",
    "            512,\n",
    "            512,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            hidden_kernel_size=1,\n",
    "            bias=False)\n",
    "        self.hyper1 = HyperConvLSTMCell(512,512,512,512)\n",
    "        self.hyper2 = HyperConvLSTMCell(128,512,512,512)\n",
    "        self.hyper3 = HyperConvLSTMCell(128,256,256,256)\n",
    "        self.hyper4 = HyperConvLSTMCell(64,128,128,128)\n",
    "        \n",
    "#         self.rnn2 = ConvLSTMCell(\n",
    "#             128,\n",
    "#             512,\n",
    "#             kernel_size=3,\n",
    "#             stride=1,\n",
    "#             padding=1,\n",
    "#             hidden_kernel_size=1,\n",
    "#             bias=False)\n",
    "#         self.rnn3 = ConvLSTMCell(\n",
    "#             128,\n",
    "#             256,\n",
    "#             kernel_size=3,\n",
    "#             stride=1,\n",
    "#             padding=1,\n",
    "#             hidden_kernel_size=3,\n",
    "#             bias=False)\n",
    "#         self.rnn4 = ConvLSTMCell(\n",
    "#             64,\n",
    "#             128,\n",
    "#             kernel_size=3,\n",
    "#             stride=1,\n",
    "#             padding=1,\n",
    "#             hidden_kernel_size=3,\n",
    "#             bias=False)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            32, 3, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, input,context, hidden1, hidden2, hidden3, hidden4):\n",
    "        x = self.conv1(input)\n",
    "\n",
    "        hidden1,x = self.hyper1(x,context[0],hidden1)\n",
    "\n",
    "        x = F.pixel_shuffle(x, 2)\n",
    "\n",
    "        hidden2,x = self.hyper2(x,context[1], hidden2)\n",
    "#         x = hidden2[0]\n",
    "        x = F.pixel_shuffle(x, 2)\n",
    "\n",
    "        hidden3,x = self.hyper3(x,context[2], hidden3)\n",
    "#         x = hidden3[0]\n",
    "        x = F.pixel_shuffle(x, 2)\n",
    "\n",
    "        hidden4,x = self.hyper4(x,context[3], hidden4)\n",
    "#         x = hidden4[0]\n",
    "        x = F.pixel_shuffle(x, 2)\n",
    "\n",
    "        x = F.tanh(self.conv2(x)) / 2\n",
    "\n",
    "        return x, hidden1, hidden2, hidden3, hidden4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderCell().cuda()\n",
    "binarizer = Binarizer().cuda()\n",
    "decoder = DecoderCell().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = optim.Adam(\n",
    "    [\n",
    "        {\n",
    "            'params': encoder.parameters()\n",
    "        },\n",
    "        {\n",
    "            'params': binarizer.parameters()\n",
    "        },\n",
    "        {\n",
    "            'params': decoder.parameters()\n",
    "        },\n",
    "    ],\n",
    "    lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "arr = np.random.randint(0,255,size=(32,32,3),dtype=np.uint8)\n",
    "im = Image.fromarray(arr).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from https://github.com/desimone/vision/blob/fb74c76d09bcc2594159613d5bdadd7d4697bb11/torchvision/datasets/folder.py\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import random\n",
    "IMG_EXTENSIONS = [\n",
    "    '.jpg',\n",
    "    '.JPG',\n",
    "    '.jpeg',\n",
    "    '.JPEG',\n",
    "    '.png',\n",
    "    '.PNG',\n",
    "    '.ppm',\n",
    "    '.PPM',\n",
    "    '.bmp',\n",
    "    '.BMP',\n",
    "]\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def readImage(path):\n",
    "    img = cv2.imread(path)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img/255.0\n",
    "    return img\n",
    "\n",
    "def default_loader(paths,root):\n",
    "    paths = [os.path.join(root,i) for i in paths]\n",
    "    main_imgs = readImage(paths[0])\n",
    "    #change this\n",
    "    motion_imgs = np.zeros((288, 352,4))\n",
    "    ctx_imgs  = [readImage(path) for path in paths[1:]]\n",
    "    ctx_imgs = np.concatenate(ctx_imgs, axis=2)\n",
    "    main_imgs = np.concatenate([main_imgs,motion_imgs], axis=2)\n",
    "    return main_imgs,ctx_imgs\n",
    "\n",
    "def crop_cv2(img, patch):\n",
    "    height, width, c = img.shape\n",
    "    start_x = random.randint(0, height - patch)\n",
    "    start_y = random.randint(0, width - patch)\n",
    "    return img[start_x : start_x + patch, start_y : start_y + patch]\n",
    "\n",
    "def np_to_torch(img):\n",
    "    img = np.swapaxes(img, 0, 1) #w, h, 9\n",
    "    img = np.swapaxes(img, 0, 2) #9, h, w\n",
    "    return torch.from_numpy(img).float()\n",
    "\n",
    "def swap(img):\n",
    "    img = np.swapaxes(img, 0, 2) #w, h, 9\n",
    "    img = np.swapaxes(img, 0, 1) #9, h, w\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_bmv_filenames(mv_dir, main_fn):\n",
    "\n",
    "    fn = main_fn.split('/')[-1][:-4]\n",
    "\n",
    "    return (os.path.join(mv_dir, fn + '_before_flow_x_0001.jpg'),\n",
    "            os.path.join(mv_dir, fn + '_before_flow_y_0001.jpg'),\n",
    "            os.path.join(mv_dir, fn + '_after_flow_x_0001.jpg'),\n",
    "            os.path.join(mv_dir, fn + '_after_flow_y_0001.jpg'))\n",
    "\n",
    "\n",
    "def get_identity_grid(shape):\n",
    "    width, height = shape\n",
    "    grid = np.zeros((width, height, 2))\n",
    "    for i in range(width):\n",
    "        for j in range(height):\n",
    "            grid[i, j, 0] = float(j) * (2.0 / (height - 1.0)) - 1.0\n",
    "            grid[i, j, 1] = float(i) * (2.0 / (width - 1.0)) - 1.0\n",
    "    return grid\n",
    "\n",
    "def get_bmv(img, fns):\n",
    "    before_x, before_y, after_x, after_y = fns\n",
    "    # get all motions for the main frame\n",
    "    # 4 bmvs for a main frame\n",
    "    bmvs = [read_bmv(before_x),\n",
    "            read_bmv(before_y),\n",
    "            read_bmv(after_x),\n",
    "            read_bmv(after_y)]\n",
    "\n",
    "    if bmvs[0] is None or bmvs[1] is None:\n",
    "        if 'ultra_video_group' in before_x:\n",
    "            # We need HW to be (16n1, 16n2).\n",
    "            bmvs[0] = np.zeros((1072, 1920, 1))\n",
    "            bmvs[1] = np.zeros((1072, 1920, 1))\n",
    "        else:\n",
    "            bmvs[0] = np.zeros((288, 352, 1))\n",
    "            bmvs[1] = np.zeros((288, 352, 1))\n",
    "    else:\n",
    "        bmvs[0] = bmvs[0] * (-2.0)\n",
    "        bmvs[1] = bmvs[1] * (-2.0)        \n",
    " \n",
    "    if bmvs[2] is None or bmvs[3] is None:\n",
    "        if 'ultra_video_group' in before_x:\n",
    "            bmvs[2] = np.zeros((1072, 1920, 1))\n",
    "            bmvs[3] = np.zeros((1072, 1920, 1))\n",
    "        else:\n",
    "            bmvs[2] = np.zeros((288, 352, 1))\n",
    "            bmvs[3] = np.zeros((288, 352, 1))\n",
    "    else:\n",
    "        bmvs[2] = bmvs[2] * (-2.0)\n",
    "        bmvs[3] = bmvs[3] * (-2.0)        \n",
    "        # bmv -256 to + 256\n",
    "    return bmvs\n",
    "\n",
    "\n",
    "\n",
    "class ImageFolder(data.Dataset):\n",
    "    \"\"\" ImageFolder can be used to load images where there are no labels.\"\"\"\n",
    "\n",
    "    def __init__(self, root, transform=None, loader=default_loader):\n",
    "        images = []\n",
    "        pickledFile = os.path.join(root,\"trainHyperTuple100.p\")\n",
    "        images = pickle.load(open(pickledFile,\"rb\"))\n",
    "        self.root = root\n",
    "        self.imgs = images\n",
    "        self.loader = loader\n",
    "\n",
    "        # for filename in os.listdir(root):\n",
    "        #     if is_image_file(filename):\n",
    "        #         images.append('{}'.format(filename))\n",
    "#         self.root = root\n",
    "#         self.imgs = images\n",
    "#         self.transform = transform\n",
    "#         self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filenames = self.imgs[index]\n",
    "        main_imgs,ctx_imgs = self.loader(filenames,self.root)\n",
    "#         croppedImgs = imgs\n",
    "        croppedImgs = crop_cv2(main_imgs,32)\n",
    "        croppedImgs = np_to_torch(croppedImgs)\n",
    "        ctx_imgs = np_to_torch(ctx_imgs)\n",
    "        return croppedImgs,ctx_imgs,filenames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_set = ImageFolder(root=\"/home_01/f20150198/datasets/ActivityNet/Crawler/Kinetics\", transform=train_transform)\n",
    "train_loader = data.DataLoader(dataset=train_set, batch_size=1, shuffle=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet(9,1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data torch.Size([1, 7, 32, 32]) context torch.Size([1, 9, 288, 352])\n",
      "prepare input crop shape torch.Size([1, 7, 32, 32]) unet shape torch.Size([1, 256, 36, 44])\n",
      "2\n",
      "before warping [torch.Size([1, 256, 36, 44]), torch.Size([1, 128, 72, 88]), torch.Size([1, 64, 144, 176])]\n",
      "[torch.Size([1, 256, 4, 4]), torch.Size([1, 128, 8, 8]), torch.Size([1, 64, 16, 16])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home_01/f20150198/miniconda2/envs/python3/lib/python3.5/site-packages/torch/nn/modules/upsampling.py:225: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home_01/f20150198/miniconda2/envs/python3/lib/python3.5/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for batch, (data,context,name) in enumerate(train_loader):\n",
    "#     cv2.imwrite(name[0][0].replace(\"outTrainImgs/\",\"main_\"),swap(data.numpy()[0])*255)\n",
    "#     cv2.imwrite(name[1][0].replace(\"outTrainImgs/\",\"ctx_\"),swap(context.numpy()[0,:3])*255)\n",
    "#     cv2.imwrite(name[2][0].replace(\"outTrainImgs/\",\"ctx_\"),swap(context.numpy()[0,3:6])*255)\n",
    "#     cv2.imwrite(name[3][0].replace(\"outTrainImgs/\",\"ctx_\"),swap(context.numpy()[0,6:9])*255)\n",
    "\n",
    "    print(\"data\",data.shape,\"context\",context.shape)\n",
    "    unet_outputs = forward_ctx(unet,context)\n",
    "    res, warped_unet_outputs = prepare_inputs(data,unet_outputs)\n",
    "    print([i.shape for i in warped_unet_outputs])\n",
    "    \n",
    "    batch_t0 = time.time()\n",
    "\n",
    "    ## init lstm state\n",
    "    encoder_h_1 = (Variable(torch.zeros(data.size(0), 256, 8, 8).cuda()),\n",
    "                   Variable(torch.zeros(data.size(0), 256, 8, 8).cuda()))\n",
    "    encoder_h_2 = (Variable(torch.zeros(data.size(0), 512, 4, 4).cuda()),\n",
    "                   Variable(torch.zeros(data.size(0), 512, 4, 4).cuda()))\n",
    "    encoder_h_3 = (Variable(torch.zeros(data.size(0), 512, 2, 2).cuda()),\n",
    "                   Variable(torch.zeros(data.size(0), 512, 2, 2).cuda()))\n",
    "\n",
    "    decoder_h_1 = (Variable(torch.zeros(data.size(0), 512 + 512, 2, 2).cuda()),\n",
    "                    Variable(torch.zeros(data.size(0), 512 + 512, 2, 2).cuda()))\n",
    "    decoder_h_2 = (Variable(torch.zeros(data.size(0), 512, 4, 4).cuda()),\n",
    "                   Variable(torch.zeros(data.size(0), 512, 4, 4).cuda()))\n",
    "    decoder_h_3 = (Variable(torch.zeros(data.size(0), 256, 8, 8).cuda()),\n",
    "                   Variable(torch.zeros(data.size(0), 256, 8, 8).cuda()))\n",
    "    decoder_h_4 = (Variable(torch.zeros(data.size(0), 128, 16, 16).cuda()),\n",
    "                   Variable(torch.zeros(data.size(0), 128, 16, 16).cuda()))\n",
    "\n",
    "    patches = Variable(data.cuda())\n",
    "\n",
    "    solver.zero_grad()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    res = patches - 0.5\n",
    "\n",
    "    context = context.cuda()\n",
    "\n",
    "    context = unet(context)\n",
    "\n",
    "    bp_t0 = time.time()\n",
    "\n",
    "    for _ in range(args.iterations):\n",
    "        encoded, encoder_h_1, encoder_h_2, encoder_h_3 = encoder(\n",
    "            res, encoder_h_1, encoder_h_2, encoder_h_3)\n",
    "\n",
    "        codes = binarizer(encoded)\n",
    "\n",
    "        output, decoder_h_1, decoder_h_2, decoder_h_3, decoder_h_4 = decoder(\n",
    "            codes,context, decoder_h_1, decoder_h_2, decoder_h_3, decoder_h_4)\n",
    "\n",
    "        res = res - output\n",
    "        losses.append(res.abs().mean())\n",
    "\n",
    "    bp_t1 = time.time()\n",
    "\n",
    "    loss = sum(losses) / args.iterations\n",
    "    loss.backward()\n",
    "\n",
    "    solver.step()\n",
    "\n",
    "    batch_t1 = time.time()\n",
    "\n",
    "    print(\n",
    "        '[TRAIN] Epoch[{}]({}/{}); Loss: {:.6f}; Backpropagation: {:.4f} sec; Batch: {:.4f} sec'.\n",
    "        format(epoch, batch + 1,\n",
    "               len(train_loader), loss.data[0], bp_t1 - bp_t0, batch_t1 -\n",
    "               batch_t0))\n",
    "    print(('{:.4f} ' * args.iterations +\n",
    "           '\\n').format(* [l.data[0] for l in losses]))\n",
    "\n",
    "    index = (epoch - 1) * len(train_loader) + batch\n",
    "\n",
    "    ## save checkpoint every 500 training steps\n",
    "    if index % 2000 == 0:\n",
    "        save(0, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transpose_to_grid(frame2):\n",
    "    # b, c, h, w\n",
    "    # b, h, c, w\n",
    "    # b, h, w, c\n",
    "    frame2 = frame2.transpose(1, 2)\n",
    "    frame2 = frame2.transpose(2, 3)\n",
    "    return frame2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_sample = nn.AvgPool2d(2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(data,unet_outputs):\n",
    "    data_arr = []\n",
    "    warped_unet_outputs = []\n",
    "    # enumerating through 2 crops of same image\n",
    "    patches = Variable(data.cuda())\n",
    "    print(\"prepare input crop shape\",data.shape,\"unet shape\",unet_outputs[0].shape)\n",
    "    res, flows = prepare_batch(patches)\n",
    "#     data_arr.append(res)\n",
    "#     frame1_arr.append(frame1)\n",
    "#     frame2_arr.append(frame2)\n",
    "    print(len(flows))\n",
    "\n",
    "    print(\"before warping\",[i.shape for i in unet_outputs])\n",
    "    wuo = warp_unet_outputs(flows, unet_outputs)\n",
    "\n",
    "#     warped_unet_output1.append(wuo1)\n",
    "#     warped_unet_output2.append(wuo2)\n",
    "\n",
    "\n",
    "# #     res = torch.cat(data_arr, dim=0)\n",
    "# #     frame1 = torch.cat(frame1_arr, dim=0)\n",
    "# #     frame2 = torch.cat(frame2_arr, dim=0)\n",
    "#     warped_unet_output1 = [torch.cat(wuos, dim=0) for wuos in zip(*warped_unet_output1)]\n",
    "#     warped_unet_output2 = [torch.cat(wuos, dim=0) for wuos in zip(*warped_unet_output2)]\n",
    "\n",
    "    return res, wuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_batch(batch):\n",
    "    res = batch - 0.5\n",
    "\n",
    "    flows = []\n",
    "    frame1, frame2 = None, None\n",
    "\n",
    "    #after x and y             \n",
    "    flow_1 = res[:, 3:5]\n",
    "    #before x and y\n",
    "    flow_2 = res[:, 5:7]\n",
    "\n",
    "    flows.append(get_flows(flow_1))\n",
    "    flows.append(get_flows(flow_2))\n",
    "    res = res[:, :3]\n",
    "\n",
    "    return res, flows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flows(flow):\n",
    "    flow_4 = down_sample(flow)\n",
    "    flow_3 = down_sample(flow_4)\n",
    "    flow_2 = down_sample(flow_3)\n",
    "\n",
    "    flow_4 = transpose_to_grid(flow_4)\n",
    "    flow_3 = transpose_to_grid(flow_3)\n",
    "    flow_2 = transpose_to_grid(flow_2)\n",
    "\n",
    "    final_grid_4 = flow_4 + 0.5\n",
    "    final_grid_3 = flow_3 + 0.5\n",
    "    final_grid_2 = flow_2 + 0.5\n",
    "\n",
    "    return [final_grid_4, final_grid_3, final_grid_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_unet_outputs(flows, unet_outputs):\n",
    "    [grid_1_4, grid_1_3, grid_1_2] = flows[0]\n",
    "    [grid_2_4, grid_2_3, grid_2_2] = flows[1]\n",
    "\n",
    "    warped_unet_outputs= []\n",
    "\n",
    "    warped_unet_outputs.append(F.grid_sample(\n",
    "        unet_outputs[0], grid_1_2, padding_mode='border'))\n",
    "\n",
    "    warped_unet_outputs.append(F.grid_sample(\n",
    "        unet_outputs[1], grid_1_3, padding_mode='border'))\n",
    "\n",
    "    warped_unet_outputs.append(F.grid_sample(\n",
    "        unet_outputs[2], grid_1_4, padding_mode='border'))\n",
    "\n",
    "    return warped_unet_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_ctx(unet, ctx_frames):\n",
    "    ctx_frames = Variable(ctx_frames.cuda()) - 0.5\n",
    "\n",
    "    unet_outputs = unet(ctx_frames)\n",
    "\n",
    "    return unet_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_identity_grid(shape):\n",
    "    width, height = shape\n",
    "    grid = np.zeros((width, height, 2))\n",
    "    for i in range(width):\n",
    "        for j in range(height):\n",
    "            grid[i, j, 0] = float(j) * (2.0 / (height - 1.0)) - 1.0\n",
    "            grid[i, j, 1] = float(i) * (2.0 / (width - 1.0)) - 1.0\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.        , -1.        ],\n",
       "        [-0.99430199, -1.        ],\n",
       "        [-0.98860399, -1.        ],\n",
       "        ...,\n",
       "        [ 0.98860399, -1.        ],\n",
       "        [ 0.99430199, -1.        ],\n",
       "        [ 1.        , -1.        ]],\n",
       "\n",
       "       [[-1.        , -0.99215686],\n",
       "        [-0.99430199, -0.99215686],\n",
       "        [-0.98860399, -0.99215686],\n",
       "        ...,\n",
       "        [ 0.98860399, -0.99215686],\n",
       "        [ 0.99430199, -0.99215686],\n",
       "        [ 1.        , -0.99215686]],\n",
       "\n",
       "       [[-1.        , -0.98431373],\n",
       "        [-0.99430199, -0.98431373],\n",
       "        [-0.98860399, -0.98431373],\n",
       "        ...,\n",
       "        [ 0.98860399, -0.98431373],\n",
       "        [ 0.99430199, -0.98431373],\n",
       "        [ 1.        , -0.98431373]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.        ,  0.98431373],\n",
       "        [-0.99430199,  0.98431373],\n",
       "        [-0.98860399,  0.98431373],\n",
       "        ...,\n",
       "        [ 0.98860399,  0.98431373],\n",
       "        [ 0.99430199,  0.98431373],\n",
       "        [ 1.        ,  0.98431373]],\n",
       "\n",
       "       [[-1.        ,  0.99215686],\n",
       "        [-0.99430199,  0.99215686],\n",
       "        [-0.98860399,  0.99215686],\n",
       "        ...,\n",
       "        [ 0.98860399,  0.99215686],\n",
       "        [ 0.99430199,  0.99215686],\n",
       "        [ 1.        ,  0.99215686]],\n",
       "\n",
       "       [[-1.        ,  1.        ],\n",
       "        [-0.99430199,  1.        ],\n",
       "        [-0.98860399,  1.        ],\n",
       "        ...,\n",
       "        [ 0.98860399,  1.        ],\n",
       "        [ 0.99430199,  1.        ],\n",
       "        [ 1.        ,  1.        ]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = get_identity_grid((256,352))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "# sub-parts of the U-Net model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class double_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            double_conv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        #  would be a nice idea if the upsampling could be learned too,\n",
    "        #  but my machine do not have enough memory to handle all those weights\n",
    "        if bilinear:\n",
    "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffX = x1.size()[2] - x2.size()[2]\n",
    "        diffY = x1.size()[3] - x2.size()[3]\n",
    "        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n",
    "                        diffY // 2, int(diffY / 2)))\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# full assembly of the sub-parts to form the complete net\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# python 3 confusing imports :(\n",
    "from unet_parts import *\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, shrink):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(n_channels, 64 // shrink)\n",
    "        self.down1 = down(64 // shrink, 128 // shrink)\n",
    "        self.down2 = down(128 // shrink, 256 // shrink)\n",
    "        self.down3 = down(256 // shrink, 512 // shrink)\n",
    "        self.down4 = down(512 // shrink, 512 // shrink)\n",
    "        self.up1 = up(1024 // shrink, 256 // shrink)\n",
    "        self.up2 = up(512 // shrink, 128 // shrink)\n",
    "        self.up3 = up(256 // shrink, 64 // shrink)\n",
    "        self.up4 = up(128 // shrink, 64 // shrink)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        out1 = self.up1(x5, x4)\n",
    "        out2 = self.up2(out1, x3)\n",
    "        out3 = self.up3(out2, x2)\n",
    "        return [out1, out2, out3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 32, 32])\n",
      "x5 torch.Size([1, 512, 2, 2])\n",
      "out1 torch.Size([1, 512, 4, 4])\n",
      "out2 torch.Size([1, 256, 8, 8])\n",
      "out3 torch.Size([1, 128, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = torch.randn(1, 3, 32, 32,dtype=torch.float)\n",
    "unet = UNet(3,1)\n",
    "b = unet(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(b.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
