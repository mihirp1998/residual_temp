{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dataset\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as LS\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.utils import _pair\n",
    "\n",
    "\n",
    "class ConvRNNCellBase(nn.Module):\n",
    "    def __repr__(self):\n",
    "        s = (\n",
    "            '{name}({input_channels}, {hidden_channels}, kernel_size={kernel_size}'\n",
    "            ', stride={stride}')\n",
    "        if self.padding != (0, ) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1, ) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        s += ', hidden_kernel_size={hidden_kernel_size}'\n",
    "        s += ')'\n",
    "        return s.format(name=self.__class__.__name__, **self.__dict__)\n",
    "\n",
    "\n",
    "class ConvLSTMCell(ConvRNNCellBase):\n",
    "    def __init__(self,\n",
    "                 input_channels,\n",
    "                 hidden_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 padding=0,\n",
    "                 dilation=1,\n",
    "                 hidden_kernel_size=1,\n",
    "                 bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.kernel_size = _pair(kernel_size)\n",
    "        self.stride = _pair(stride)\n",
    "        self.padding = _pair(padding)\n",
    "        self.dilation = _pair(dilation)\n",
    "\n",
    "        self.hidden_kernel_size = _pair(hidden_kernel_size)\n",
    "\n",
    "        hidden_padding = _pair(hidden_kernel_size // 2)\n",
    "\n",
    "        gate_channels = 4 * self.hidden_channels\n",
    "        self.conv_ih = nn.Conv2d(\n",
    "            in_channels=self.input_channels,\n",
    "            out_channels=gate_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "            dilation=self.dilation,\n",
    "            bias=bias)\n",
    "\n",
    "        self.conv_hh = nn.Conv2d(\n",
    "            in_channels=self.hidden_channels,\n",
    "            out_channels=gate_channels,\n",
    "            kernel_size=hidden_kernel_size,\n",
    "            stride=1,\n",
    "            padding=hidden_padding,\n",
    "            dilation=1,\n",
    "            bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv_ih.reset_parameters()\n",
    "        self.conv_hh.reset_parameters()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        hx, cx = hidden\n",
    "        gates = self.conv_ih(input) + self.conv_hh(hx)\n",
    "#         print(\"gates\",gates.shape)\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "\n",
    "        ingate = F.sigmoid(ingate)\n",
    "        forgetgate = F.sigmoid(forgetgate)\n",
    "        cellgate = F.tanh(cellgate)\n",
    "        outgate = F.sigmoid(outgate)\n",
    "\n",
    "        cy = (forgetgate * cx) + (ingate * cellgate)\n",
    "        hy = outgate * F.tanh(cy)\n",
    "\n",
    "        return hy, cy\n",
    "\n",
    "\n",
    "class HyperConvLSTMCell(ConvRNNCellBase):\n",
    "    def __init__(self,input_channels,main_num_units,hyper_unit,context_input_channels,hyper_embedding = 128):\n",
    "        super(HyperConvLSTMCell, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.num_units = main_num_units\n",
    "        self.hyper_num_unit = hyper_unit\n",
    "        self.hyper_embedding =hyper_embedding\n",
    "        self.gate_params  = self.num_units * 4\n",
    "        self.context_input_channels = context_input_channels\n",
    "        self.hyper_input_units = context_input_channels+ main_num_units  \n",
    "\n",
    " \n",
    "        # print(self.hyper_input_units,self.hyper_num_unit)\n",
    "\n",
    "        self.hyper_cell = ConvLSTMCell(\n",
    "            self.hyper_input_units,\n",
    "            self.hyper_num_unit,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            hidden_kernel_size=1,\n",
    "            bias=False)\n",
    "        \n",
    "        self.hyper_cell = self.hyper_cell.cuda()\n",
    "\n",
    "#         self.temp_conv = nn.Conv2d(\n",
    "#                     input_channels , \n",
    "#                     input_channels, \n",
    "#                     kernel_size=3, stride=2, padding=1, bias=False)\n",
    "#         self.temp_conv = self.temp_conv.cuda()\n",
    "\n",
    "        self.conv_z_input  = nn.Conv2d(self.hyper_num_unit, self.hyper_embedding, \n",
    "                        kernel_size=1, stride=1, padding=0, bias=False).cuda()\n",
    "\n",
    "        self.conv_z_state  = nn.Conv2d(self.hyper_num_unit, self.hyper_embedding, \n",
    "                        kernel_size=1, stride=1, padding=0, bias=False).cuda()\n",
    "\n",
    "        self.conv_alpha_input  = nn.Conv2d(self.hyper_embedding , self.gate_params, \n",
    "                    kernel_size=1, stride=1, padding=0, bias=False).cuda()\n",
    "\n",
    "        self.conv_alpha_state  = nn.Conv2d(self.hyper_embedding , self.gate_params, \n",
    "                    kernel_size=1, stride=1, padding=0, bias=False).cuda()\n",
    "\n",
    "        \n",
    "        \n",
    "        self.conv_transform_gates_input  = nn.Conv2d(self.input_channels , self.gate_params, \n",
    "                    kernel_size=3, stride=1, padding=1, bias=False).cuda()\n",
    "\n",
    "        self.conv_transform_gates_states  = nn.Conv2d(self.num_units , self.gate_params, \n",
    "                    kernel_size=1, stride=1, padding=0, bias=False).cuda()\n",
    "\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.hyper_cell.reset_parameters()\n",
    "        self.conv_z_input.reset_parameters()\n",
    "        self.conv_z_state.reset_parameters()\n",
    "        self.conv_alpha_input.reset_parameters()\n",
    "        self.conv_alpha_state.reset_parameters()\n",
    "        self.conv_transform_gates_input.reset_parameters()\n",
    "        self.conv_transform_gates_states.reset_parameters()\n",
    "\n",
    "    def hyper_norm_input(self,input_layer,hyper_h):\n",
    "        zw = self.conv_z_input(hyper_h)\n",
    "        alpha = self.conv_alpha_input(zw)\n",
    "\n",
    "        result = input_layer * alpha\n",
    "        \n",
    "        return result\n",
    "\n",
    "    \n",
    "    def hyper_norm_state(self,input_layer,hyper_h):\n",
    "        zw = self.conv_z_state(hyper_h)\n",
    "        \n",
    "        alpha = self.conv_alpha_state(zw)\n",
    "        result = input_layer * alpha\n",
    "        \n",
    "        return result    \n",
    "    # def reset_parameters(self):\n",
    "    #     self.conv_ih.reset_parameters()\n",
    "    #     self.conv_hh.reset_parameters()\n",
    "\n",
    "    def forward(self, input,context, hidden):\n",
    "        h,c = hidden\n",
    "        main_h = h[:,:self.num_units]\n",
    "        main_c = c[:,:self.num_units]\n",
    "        hyper_h = h[:,self.num_units:]\n",
    "        hyper_c = h[:,self.num_units:]\n",
    "        # print(\"input shape \",input.shape)\n",
    "        hyper_states = (hyper_h,hyper_c)\n",
    "        # if self.encoder:\n",
    "        #     input = self.temp_conv(input)\n",
    "        hyper_input = torch.cat([context,main_h],dim=1)\n",
    "        # print(\"hyper shape \",hyper_input.shape,hyper_states[0].shape)\n",
    "\n",
    "        #print(hyper_input.shape,hyper_states[0].shape)\n",
    "        hyper_h,hyper_c = self.hyper_cell(hyper_input,hyper_states)\n",
    "\n",
    "        input_below_ = self.conv_transform_gates_input(input)\n",
    "        input_below_ = self.hyper_norm_input(input_below_,hyper_h)\n",
    "\n",
    "        state_below_ = self.conv_transform_gates_states(main_h)\n",
    "        state_below_ = self.hyper_norm_state(state_below_,hyper_h)\n",
    "\n",
    "        lstm_matrix = input_below_ + state_below_\n",
    "\n",
    "        i,j,f,o = lstm_matrix.chunk(4,1)\n",
    "\n",
    "\n",
    "        new_main_c = (self.sigmoid(f)*main_c) + (self.sigmoid(i)*self.tanh(j))\n",
    "        new_main_h = self.tanh(new_main_c)* self.sigmoid(o)\n",
    "\n",
    "        new_total_h =torch.cat([new_main_h,hyper_h],dim=1)\n",
    "\n",
    "        new_total_c = torch.cat([new_main_c,hyper_c],dim=1)\n",
    "\n",
    "\n",
    "        return (new_total_h,new_total_c),new_main_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from modules import Sign\n",
    "\n",
    "\n",
    "class EncoderCell(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCell, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.rnn1 = ConvLSTMCell(\n",
    "            64,\n",
    "            256,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            hidden_kernel_size=1,\n",
    "            bias=False)\n",
    "        self.rnn2 = ConvLSTMCell(\n",
    "            256,\n",
    "            512,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            hidden_kernel_size=1,\n",
    "            bias=False)\n",
    "        self.rnn3 = ConvLSTMCell(\n",
    "            512,\n",
    "            512,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            hidden_kernel_size=1,\n",
    "            bias=False)\n",
    "\n",
    "    def forward(self, input, hidden1, hidden2, hidden3):\n",
    "        x = self.conv(input)\n",
    "#         print(x.shape)\n",
    "        hidden1 = self.rnn1(x, hidden1)\n",
    "        x = hidden1[0]\n",
    "\n",
    "        hidden2 = self.rnn2(x, hidden2)\n",
    "        x = hidden2[0]\n",
    "\n",
    "        hidden3 = self.rnn3(x, hidden3)\n",
    "        x = hidden3[0]\n",
    "\n",
    "        return x, hidden1, hidden2, hidden3\n",
    "\n",
    "\n",
    "class Binarizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Binarizer, self).__init__()\n",
    "        self.conv = nn.Conv2d(512, 32, kernel_size=1, bias=False)\n",
    "        self.sign = Sign()\n",
    "\n",
    "    def forward(self, input):\n",
    "        feat = self.conv(input)\n",
    "        x = F.tanh(feat)\n",
    "        return self.sign(x)\n",
    "    \n",
    "# input_channels,main_num_units,hyper_unit,context_input_channels\n",
    "\n",
    "class DecoderCell(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderCell, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            32, 512, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.rnn1 = ConvLSTMCell(\n",
    "            512,\n",
    "            512,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            hidden_kernel_size=1,\n",
    "            bias=False)\n",
    "        self.hyper1 = HyperConvLSTMCell(512,512,512,512)\n",
    "        self.hyper2 = HyperConvLSTMCell(128,512,512,512)\n",
    "        self.hyper3 = HyperConvLSTMCell(128,256,256,256)\n",
    "        self.hyper4 = HyperConvLSTMCell(64,128,128,128)\n",
    "        \n",
    "#         self.rnn2 = ConvLSTMCell(\n",
    "#             128,\n",
    "#             512,\n",
    "#             kernel_size=3,\n",
    "#             stride=1,\n",
    "#             padding=1,\n",
    "#             hidden_kernel_size=1,\n",
    "#             bias=False)\n",
    "#         self.rnn3 = ConvLSTMCell(\n",
    "#             128,\n",
    "#             256,\n",
    "#             kernel_size=3,\n",
    "#             stride=1,\n",
    "#             padding=1,\n",
    "#             hidden_kernel_size=3,\n",
    "#             bias=False)\n",
    "#         self.rnn4 = ConvLSTMCell(\n",
    "#             64,\n",
    "#             128,\n",
    "#             kernel_size=3,\n",
    "#             stride=1,\n",
    "#             padding=1,\n",
    "#             hidden_kernel_size=3,\n",
    "#             bias=False)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            32, 3, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, input,context, hidden1, hidden2, hidden3, hidden4):\n",
    "        x = self.conv1(input)\n",
    "\n",
    "        hidden1,x = self.hyper1(x,context[0],hidden1)\n",
    "\n",
    "        x = F.pixel_shuffle(x, 2)\n",
    "\n",
    "        hidden2,x = self.hyper2(x,context[1], hidden2)\n",
    "#         x = hidden2[0]\n",
    "        x = F.pixel_shuffle(x, 2)\n",
    "\n",
    "        hidden3,x = self.hyper3(x,context[2], hidden3)\n",
    "#         x = hidden3[0]\n",
    "        x = F.pixel_shuffle(x, 2)\n",
    "\n",
    "        hidden4,x = self.hyper4(x,context[3], hidden4)\n",
    "#         x = hidden4[0]\n",
    "        x = F.pixel_shuffle(x, 2)\n",
    "\n",
    "        x = F.tanh(self.conv2(x)) / 2\n",
    "\n",
    "        return x, hidden1, hidden2, hidden3, hidden4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderCell().cuda()\n",
    "binarizer = Binarizer().cuda()\n",
    "decoder = DecoderCell().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = optim.Adam(\n",
    "    [\n",
    "        {\n",
    "            'params': encoder.parameters()\n",
    "        },\n",
    "        {\n",
    "            'params': binarizer.parameters()\n",
    "        },\n",
    "        {\n",
    "            'params': decoder.parameters()\n",
    "        },\n",
    "    ],\n",
    "    lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "arr = np.random.randint(0,255,size=(32,32,3),dtype=np.uint8)\n",
    "im = Image.fromarray(arr).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from https://github.com/desimone/vision/blob/fb74c76d09bcc2594159613d5bdadd7d4697bb11/torchvision/datasets/folder.py\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import random\n",
    "IMG_EXTENSIONS = [\n",
    "    '.jpg',\n",
    "    '.JPG',\n",
    "    '.jpeg',\n",
    "    '.JPEG',\n",
    "    '.png',\n",
    "    '.PNG',\n",
    "    '.ppm',\n",
    "    '.PPM',\n",
    "    '.bmp',\n",
    "    '.BMP',\n",
    "]\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def readImage(path):\n",
    "    img = cv2.imread(path)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img/255.0\n",
    "    return img\n",
    "\n",
    "def default_loader(paths,root):\n",
    "    paths = [os.path.join(root,i) for i in paths]\n",
    "    imgs  = [readImage(path) for path in paths]\n",
    "    imgs = np.concatenate(imgs, axis=2)\n",
    "    return imgs\n",
    "\n",
    "def crop_cv2(img, patch):\n",
    "    height, width, c = img.shape\n",
    "    start_x = random.randint(0, height - patch)\n",
    "    start_y = random.randint(0, width - patch)\n",
    "    return img[start_x : start_x + patch, start_y : start_y + patch]\n",
    "\n",
    "def np_to_torch(img):\n",
    "    img = np.swapaxes(img, 0, 1) #w, h, 9\n",
    "    img = np.swapaxes(img, 0, 2) #9, h, w\n",
    "    return torch.from_numpy(img).float()\n",
    "\n",
    "def swap(img):\n",
    "    img = np.swapaxes(img, 0, 2) #w, h, 9\n",
    "    img = np.swapaxes(img, 0, 1) #9, h, w\n",
    "    return img\n",
    "\n",
    "\n",
    "class ImageFolder(data.Dataset):\n",
    "    \"\"\" ImageFolder can be used to load images where there are no labels.\"\"\"\n",
    "\n",
    "    def __init__(self, root, transform=None, loader=default_loader):\n",
    "        images = []\n",
    "        pickledFile = os.path.join(root,\"hypertuple.p\")\n",
    "        images = pickle.load(open(pickledFile,\"rb\"))\n",
    "        self.root = root\n",
    "        self.imgs = images\n",
    "        self.loader = loader\n",
    "\n",
    "        # for filename in os.listdir(root):\n",
    "        #     if is_image_file(filename):\n",
    "        #         images.append('{}'.format(filename))\n",
    "#         self.root = root\n",
    "#         self.imgs = images\n",
    "#         self.transform = transform\n",
    "#         self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filenames = self.imgs[index]\n",
    "        imgs = self.loader(filenames,self.root)\n",
    "#         croppedImgs = imgs\n",
    "        croppedImgs = crop_cv2(imgs,32)\n",
    "        croppedImgs = np_to_torch(croppedImgs)\n",
    "        image = croppedImgs[:3]\n",
    "        ctx_image = croppedImgs[3:]\n",
    "        return image,ctx_image,filenames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_set = ImageFolder(root=\"/home_01/f20150198/datasets/ActivityNet/Crawler/Kinetics\", transform=train_transform)\n",
    "train_loader = data.DataLoader(dataset=train_set, batch_size=1, shuffle=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5, 3, 288, 352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data torch.Size([1, 3, 32, 32]) context torch.Size([1, 9, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for batch, (data,context,name) in enumerate(train_loader):\n",
    "#     cv2.imwrite(name[0][0].replace(\"outTrainImgs/\",\"main_\"),swap(data.numpy()[0])*255)\n",
    "#     cv2.imwrite(name[1][0].replace(\"outTrainImgs/\",\"ctx_\"),swap(context.numpy()[0,:3])*255)\n",
    "#     cv2.imwrite(name[2][0].replace(\"outTrainImgs/\",\"ctx_\"),swap(context.numpy()[0,3:6])*255)\n",
    "#     cv2.imwrite(name[3][0].replace(\"outTrainImgs/\",\"ctx_\"),swap(context.numpy()[0,6:9])*255)\n",
    "\n",
    "    print(\"data\",data.shape,\"context\",context.shape)\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     batch_t0 = time.time()\n",
    "\n",
    "#     ## init lstm state\n",
    "#     encoder_h_1 = (Variable(torch.zeros(data.size(0), 256, 8, 8).cuda()),\n",
    "#                    Variable(torch.zeros(data.size(0), 256, 8, 8).cuda()))\n",
    "#     encoder_h_2 = (Variable(torch.zeros(data.size(0), 512, 4, 4).cuda()),\n",
    "#                    Variable(torch.zeros(data.size(0), 512, 4, 4).cuda()))\n",
    "#     encoder_h_3 = (Variable(torch.zeros(data.size(0), 512, 2, 2).cuda()),\n",
    "#                    Variable(torch.zeros(data.size(0), 512, 2, 2).cuda()))\n",
    "\n",
    "#     decoder_h_1 = (Variable(torch.zeros(data.size(0), 512 + 512, 2, 2).cuda()),\n",
    "#                     Variable(torch.zeros(data.size(0), 512 + 512, 2, 2).cuda()))\n",
    "#     decoder_h_2 = (Variable(torch.zeros(data.size(0), 512, 4, 4).cuda()),\n",
    "#                    Variable(torch.zeros(data.size(0), 512, 4, 4).cuda()))\n",
    "#     decoder_h_3 = (Variable(torch.zeros(data.size(0), 256, 8, 8).cuda()),\n",
    "#                    Variable(torch.zeros(data.size(0), 256, 8, 8).cuda()))\n",
    "#     decoder_h_4 = (Variable(torch.zeros(data.size(0), 128, 16, 16).cuda()),\n",
    "#                    Variable(torch.zeros(data.size(0), 128, 16, 16).cuda()))\n",
    "\n",
    "#     patches = Variable(data.cuda())\n",
    "\n",
    "#     solver.zero_grad()\n",
    "\n",
    "#     losses = []\n",
    "\n",
    "#     res = patches - 0.5\n",
    "\n",
    "#     context = context.cuda()\n",
    "\n",
    "#     context = unet(context)\n",
    "\n",
    "#     bp_t0 = time.time()\n",
    "\n",
    "#     for _ in range(args.iterations):\n",
    "#         encoded, encoder_h_1, encoder_h_2, encoder_h_3 = encoder(\n",
    "#             res, encoder_h_1, encoder_h_2, encoder_h_3)\n",
    "\n",
    "#         codes = binarizer(encoded)\n",
    "\n",
    "#         output, decoder_h_1, decoder_h_2, decoder_h_3, decoder_h_4 = decoder(\n",
    "#             codes,context, decoder_h_1, decoder_h_2, decoder_h_3, decoder_h_4)\n",
    "\n",
    "#         res = res - output\n",
    "#         losses.append(res.abs().mean())\n",
    "\n",
    "#     bp_t1 = time.time()\n",
    "\n",
    "#     loss = sum(losses) / args.iterations\n",
    "#     loss.backward()\n",
    "\n",
    "#     solver.step()\n",
    "\n",
    "#     batch_t1 = time.time()\n",
    "\n",
    "#     print(\n",
    "#         '[TRAIN] Epoch[{}]({}/{}); Loss: {:.6f}; Backpropagation: {:.4f} sec; Batch: {:.4f} sec'.\n",
    "#         format(epoch, batch + 1,\n",
    "#                len(train_loader), loss.data[0], bp_t1 - bp_t0, batch_t1 -\n",
    "#                batch_t0))\n",
    "#     print(('{:.4f} ' * args.iterations +\n",
    "#            '\\n').format(* [l.data[0] for l in losses]))\n",
    "\n",
    "#     index = (epoch - 1) * len(train_loader) + batch\n",
    "\n",
    "#     ## save checkpoint every 500 training steps\n",
    "#     if index % 2000 == 0:\n",
    "#         save(0, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class double_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            double_conv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        #  would be a nice idea if the upsampling could be learned too,\n",
    "        #  but my machine do not have enough memory to handle all those weights\n",
    "        if bilinear:\n",
    "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffX = x1.size()[2] - x2.size()[2]\n",
    "        diffY = x1.size()[3] - x2.size()[3]\n",
    "        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n",
    "                        diffY // 2, int(diffY / 2)))\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, shrink):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(n_channels, 64 // shrink)\n",
    "        self.down1 = down(64 // shrink, 128 // shrink)\n",
    "        self.down2 = down(128 // shrink, 256 // shrink)\n",
    "        self.down3 = down(256 // shrink, 512 // shrink)\n",
    "        self.down4 = down(512 // shrink, 512 // shrink)\n",
    "        self.down5 = down(512 // shrink, 512 // shrink)\n",
    "        self.up1 = up(1024 // shrink, 512 // shrink)\n",
    "        self.up2 = up(768 // shrink, 256 // shrink)\n",
    "        self.up3 = up(384 // shrink, 128 // shrink)\n",
    "        self.up4 = up(128 // shrink, 64 // shrink)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        print(x1.shape)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        print(\"x5\",x5.shape)\n",
    "        out1 = self.up1(x5, x4)\n",
    "        print(\"out1\",out1.shape)\n",
    "        out2 = self.up2(out1, x3)\n",
    "        print(\"out2\",out2.shape)\n",
    "        out3 = self.up3(out2, x2)\n",
    "        print(\"out3\",out3.shape)\n",
    "        return [x5,out1,out2,out3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 32, 32])\n",
      "x5 torch.Size([1, 512, 2, 2])\n",
      "out1 torch.Size([1, 512, 4, 4])\n",
      "out2 torch.Size([1, 256, 8, 8])\n",
      "out3 torch.Size([1, 128, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = torch.randn(1, 3, 32, 32,dtype=torch.float)\n",
    "unet = UNet(3,1)\n",
    "b = unet(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(b.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
